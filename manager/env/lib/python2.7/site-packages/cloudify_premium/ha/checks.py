########
# Copyright (c) 2016 GigaSpaces Technologies Ltd. All rights reserved
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#        http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
#    * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#    * See the License for the specific language governing permissions and
#    * limitations under the License.

"""Health checks used to determine services state.

This module defines a "check runner" which is a daemon that periodically
runs the registered checks. The checks are typically related to a consul
TTL type check, so the service needs to keep updating it so that it stays
passing.

We use a separate service instead of consul's built-in check runner
(ie. instead of using checks with type "script") primarily to allow
asynchronous updates, ie. when a check is known passing, or known failing
(eg. immediately after a failover, we know that the newly-elected master's
services are up - because it was just elected - so we can immediately
mark its health checks as passing, not wait for consul to run the checks
again).
Those asynchronous updates are only available with TTL checks, not with
script checks.

Each check is a python function - if it returns, the check is considered
passing; if it throws an exception - failing.

Please note that all the checks need to also be registered with consul's
HTTP API, which happens during configuration.


"""
from __future__ import absolute_import

import time
import logging
import requests
import threading
import logging.config
from consul import ConsulException

from cloudify_premium.ha import (cluster_status,
                                 consul,
                                 database,
                                 logger_config,
                                 node_status,
                                 services,
                                 systemd,
                                 utils)

logger = logging.getLogger(__name__)


class _Check(utils._WithConsulClient):
    def __init__(self, name, func, interval=3, *args, **kwargs):
        super(_Check, self).__init__(*args, **kwargs)
        self._func = func
        self.interval = interval
        self.name = name
        self._passing = False

    @property
    def check_id(self):
        return node_status['checks'][self.name]

    def run(self):
        try:
            self()
        except (requests.exceptions.RequestException, ConsulException):
            # error when connecting consul - there's no point trying to
            # update the check state, if consul is down; simply do nothing
            # and wait for the next iteration
            pass
        except Exception:
            self.mark_failing()
            raise
        else:
            self.mark_passing()

    def mark_passing(self):
        self.consul_client.agent.check.ttl_pass(self.check_id)
        if not self._passing:
            logger.debug('{0} passing'.format(self.name))
            self._passing = True

    def mark_failing(self):
        self.consul_client.agent.check.ttl_fail(self.check_id)
        if self._passing:
            logger.debug('{0} failing'.format(self.name))
            self._passing = False

    def __call__(self, *args, **kwargs):
        return self._func(*args, **kwargs)


def check(name=None, **kwargs):
    def _decorator(f):
        check_name = name or f.__name__
        check = _Check(check_name, f, **kwargs)
        return check
    return _decorator


@check()
def check_local_db():
    """Check if the database on localhost is up and running.

    This will be called from a service health check.
    """
    # any error will just be propagated up, resulting in a error return code,
    # ie. not a passing health check
    with database.database_connection(
            host='127.0.0.1', **utils.cloudify_database_credentials()) as conn:
        with conn.cursor() as cur:
            cur.execute('SELECT 1')
            cur.fetchall()


@check()
def check_db_following():
    """Check if the local database is replicating from the current master."""
    db = database.Database()
    # check against the .next_master, so that we're already marked as failing
    # during a switch
    db_master = cluster_status.next_master
    while True:
        if node_status['name'] == db_master:
            if not db.replication.is_already_master():
                raise RuntimeError('Not master')
            break
        master_details = cluster_status.nodes[db_master]
        master_ip = master_details['host_ip']
        is_following = db.replication.is_already_following(master_ip)
        # the `is_already_following` call might have taken a long time to
        # run (up to connect_timeout - if the master is unreachable -
        # 3 seconds); the master might have changed in the meantime - if
        # so, try again with the current one
        current_db_master = cluster_status.next_master
        if current_db_master == db_master:
            if is_following:
                break
            raise RuntimeError('Not following {0}'.format(master_ip))
        # db_master changed during following check - loop again
        db_master = current_db_master


@check(interval=15)
def check_manager_services():
    name = node_status['name']
    master = cluster_status.master
    next_master = cluster_status.next_master

    for service in services.MASTER_ONLY_SERVICES:
        service_up = service.status() == 0

        # the current master must have all the services running
        if name == master and not service_up:
            raise RuntimeError('{0} not up'.format(service))

        # only the next_master can have the services running - which is
        # the current master, or the newly-chosen one during a failover
        if name != next_master and service_up:
            raise RuntimeError('{0} up'.format(service))
        # note that it's not an error for the next_master to have services
        # not running - they might have not started yet

    # services that need to be running on both the master and the replicas
    for service in services.MASTER_REPLICA_SERVICES:
        service_up = service.status() == 0
        if not service_up:
            raise RuntimeError('{0} not up'.format(service))


@check(interval=10)
def heartbeat_check():
    """If consul is online, update our heartbeat record, otherwise fail.

    Note that this check failing is usually a noop except in race-condition
    cases because if consul is offline, then we can't set the check status to
    failing anyway. It will however make consul wait until this check
    succeeds before marking the node as healthy again.
    """
    consul_client = consul.get_consul_client()
    consul_client.kv.put(
        'heartbeat/{0}'.format(node_status['name']),
        '{0}'.format(time.time())
    )


# checks to be run by the Checker
CHECKS = [
    check_local_db,
    check_db_following,
    check_manager_services,
    heartbeat_check
]


class Checker(object):
    def __init__(self):
        self._checks = CHECKS
        self._threads = None

    def run(self):
        if self._threads is not None:
            raise RuntimeError('Already started!')
        self._threads = [threading.Thread(target=self._run_check,
                                          args=(check,))
                         for check in self._checks]
        for thread in self._threads:
            thread.setDaemon(True)
            thread.start()
        while True:
            time.sleep(1)

    def _run_check(self, check):
        while True:
            try:
                check.run()
            except Exception:
                pass
            time.sleep(check.interval)


def check_runner():
    logging.config.dictConfig(logger_config.make_logger_config('check_runner'))
    logger_config
    checker = Checker()
    checker.run()


class CheckRunner(systemd.SystemdManaged):
    unit_source = 'resources/cloudify-check-runner.service'
    service_name = 'check-runner'
    user = utils.CLOUDIFY_USER
