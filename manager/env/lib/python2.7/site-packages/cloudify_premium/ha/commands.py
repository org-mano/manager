########
# Copyright (c) 2016 GigaSpaces Technologies Ltd. All rights reserved
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#        http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
#    * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#    * See the License for the specific language governing permissions and
#    * limitations under the License.

import json
import click
import logging

import logging.config
from functools import wraps

from cloudify_premium.ha import (agents,
                                 checks,
                                 cluster_status,
                                 consul,
                                 database,
                                 logger_config,
                                 minority,
                                 node_status,
                                 services,
                                 sudo,
                                 syncthing,
                                 utils,
                                 watch_handlers)


def pass_logger(logger_name=None):
    """Create a separate logger for commands.

    Logging messages from each command under a different name allows to
    better understand what message was sent by which command.
    """
    def _decorator(f):
        full_logger_name = 'commands.{0}'.format(logger_name or f.__name__)

        @wraps(f)
        def wrapper(*args, **kwargs):
            logging.config.dictConfig(
                logger_config.make_logger_config(full_logger_name))
            new_logger = logging.getLogger(full_logger_name)
            return f(logger=new_logger, *args, **kwargs)
        return wrapper
    return _decorator


def error_handled(use_status=False, log=True):
    """Logs exceptions and stores them in the node_status storage.
    Useful only for cli commands, to catch exceptions that weren't handled
    otherwise. Storing the error in node_status allows showing them to the
    user, who might not be watching the logs (because they're running
    the command via a REST call).
    """
    def _decorator(f):
        @wraps(f)
        def _wrapper(*args, **kwargs):
            try:
                return f(*args, **kwargs)
            except Exception as e:
                if log:
                    logger = kwargs['logger']
                    logger.exception('Exception in {0}(*{1}, **{2})'
                                     .format(f.__name__, args, kwargs))
                    node_status['error'] = str(e)
                raise
        return _wrapper
    return _decorator


def _parse_config(ctx, param, value):
    with open(value) as f:
        return json.load(f)


@click.command()
@click.option('--config', callback=_parse_config, required=True)
@pass_logger('create_cluster_node')
@error_handled(use_status=True)
def create_cluster_node(logger, config):
    logger.info('Configuring the Cloudify Manager cluster')
    node_status['initialized'] = False

    utils.create_cluster_group()
    utils.make_file_writable(logger_config.LOGFILE)
    utils.make_file_writable(utils.STATUS_STORAGE_PATH)

    if 'name' in node_status \
            and node_status['name'] != config['node_name']:
        logger.warning('Node name changed during retry: using saved {} '
                       'instead of the passed in {}'
                       .format(node_status['name'], config['node_name']))
        config['node_name'] = node_status['name']

    utils.deploy_logrotate(logger_config.LOGFILE)
    node_status.update({
        'ip': config['host_ip'],
        'name': config['node_name'],
        'checks': {}
    })
    sudo.deploy_script()

    iptables_restore = services.IPtablesRestoreService()
    iptables_restore.configure()

    logger.info('Configuring Consul')
    consul.configure(**config)

    logger.info('Starting Consul')
    consul.start(**config)
    logger.info('Joined the Consul cluster')

    cluster_status.nodes[node_status['name']] = {
        'name': node_status['name'],
        'host_ip': node_status['ip'],
        'initialized': False
    }

    if not cluster_status.master:
        # no cluster master declared yet, so this node declares itself the
        # master. Make sure to use CAS so that there's no race conditions in
        # case of parallel bootstrap and join on several machines
        cluster_status.master = node_status['name']
        cluster_status.next_master = node_status['name']

    logger.info('Configuring database')
    database.configure(**config)

    # this retrieves the bootstrap context from the database, so it needs
    # to run before starting the replicated db - while the original is
    # still running
    agents.configure(**config)
    agents.store_agent_context()

    logger.info('Starting database')
    database.start(**config)

    if not utils.is_master():
        logger.info('Follower node: disallowing REST requests')
        for service in reversed(services.MASTER_ONLY_SERVICES):
            service.disable()
            service.stop()

    services.deploy_check()

    logger.info('Configuring filesystem replication')
    syncthing.configure(**config)

    logger.info('Starting filesystem replication')
    syncthing.start(**config)

    node_status['initialized'] = True
    cluster_status.nodes[node_status['name']] = {
        'initialized': True,
    }

    consul_service = consul.Consul()
    # reloading consul here enables all the watches registered during setup
    consul_service.reload()

    check_runner = checks.CheckRunner()
    check_runner.configure()
    check_runner.start()

    handler_runner = watch_handlers.HandlerRunner()
    handler_runner.configure()
    handler_runner.start()

    # if the cluster we're joined is in minority mode, also fence out the
    # same nodes that are fenced out on the master
    if cluster_status.disconnected_nodes:
        disconnected_addrs = {
            node['host_ip']
            for node in cluster_status.disconnected_nodes.values()}
        minority.block_consul(disconnected_addrs)

    if utils.is_master():
        agents.update_agent_context()
    logger.info('HA Cluster configuration complete')


@click.command()
@pass_logger('consul_watcher')
@click.option('--check-interval-seconds', default=1)
@error_handled()
def consul_watcher(logger, check_interval_seconds):
    minority.watch_consul(logger=logger,
                          check_interval_seconds=check_interval_seconds)


@click.command()
@pass_logger('recovery_watcher')
def recovery_watcher(logger):
    logger.info('starting recovery watcher')
    minority.watch_recovery(logger)
    logger.info('done')
