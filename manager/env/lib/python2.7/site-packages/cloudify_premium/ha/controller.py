########
# Copyright (c) 2016 GigaSpaces Technologies Ltd. All rights reserved
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#        http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
#    * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#    * See the License for the specific language governing permissions and
#    * limitations under the License.

import os
import sys
import json
import logging
import requests
import tempfile
import itertools

from functools import wraps
from flask_security import current_user

from manager_rest.rest.responses_v2 import ListResponse
from manager_rest.storage import get_storage_manager, models
from manager_rest.manager_exceptions import (ConflictError,
                                             UnauthorizedError,
                                             ManagerException,
                                             NotFoundError)

from . import (consul,
               database,
               minority,
               ssl,
               syncthing,
               systemd,
               node_status,
               utils,
               cluster_status as cluster_status_storage)

CLUSTER_SYSTEMD_UNIT_NAME = 'ha-cluster'
REQUIRED_PORTS = [
    database.DB_PORT,
    syncthing.SYNCTHING_REPLICATION_PORT,
    # consul ports aren't configurable right now - no way to share config
    # across the cluster before consul is configured
    8300,
    8301,
    8500,
]


class ClusterConfigurationError(ManagerException):
    """An error has happened when configuring the cluster"""
    CLUSTER_CONFIGURATION_ERROR_CODE = 'cluster_configuration_error'

    def __init__(self, *args, **kwargs):
        super(ClusterConfigurationError, self).__init__(
            400, ClusterConfigurationError.CLUSTER_CONFIGURATION_ERROR_CODE,
            *args, **kwargs)


def admin_only(f):
    @wraps(f)
    def _wrapper(*args, **kwargs):
        if not current_user.is_admin:
            raise UnauthorizedError('Only admin users are allowed to configure'
                                    'cluster nodes')
        return f(*args, **kwargs)
    return _wrapper


class ClusterController(object):
    """
    The public-facing API of the cluster.

    This controller contains functions for querying and updating the
    state of the cluster and the current node.
    """

    def __init__(self, logger=None):
        self._logger = logger

    @property
    def logger(self):
        if self._logger is None:
            self._logger = logging.getLogger(__name__)
        return self._logger

    # Public methods for controlling the cluster; typically called by
    # the REST API resources, however standalone use is also possible.
    def cluster_status(self, _include=None, filters=None):
        """
        Current status of the cluster as seen from this node.
        """
        self.logger.debug('Getting cluster status: include={0}, filters={1}'
                          .format(_include, filters))

        response = {
            'initialized': node_status.get('initialized', False),
            'error': node_status.get('error')
        }

        if _include:
            if 'logs' in _include:
                response['logs'] = self._load_logs(**filters)

        consul_client = consul.get_consul_client()
        try:
            consul_leader = consul_client.status.leader()
        except requests.exceptions.ConnectionError:
            self.logger.debug('get_status: consul error. Status: %r', response)
            return response

        if not consul_leader:
            self.logger.debug('get_status: no consul leader')
            response['consul'] = {'leader': None}
            return response

        try:
            master = cluster_status_storage.nodes[
                cluster_status_storage.master]['host_ip']
        except KeyError:
            master = None

        response.update({
            'consul': {
                'leader': consul_leader
            },
            'master': master
        })

        self.logger.debug('Current cluster status: {0}'.format(response))
        return response

    @admin_only
    def start(self, config):
        config['bootstrap_cluster'] = True
        self.logger.info('Creating a new HA cluster with config: {0}'
                         .format(config))

        return self._start_cluster_member(config)

    @admin_only
    def join(self, config):
        self._verify_no_data()

        config['bootstrap_cluster'] = False
        join_ips = config['join_addrs']
        reachable_ips = []
        for join_ip in join_ips:
            for port in REQUIRED_PORTS:
                try:
                    utils.wait_for_port(join_ip, port, retries=5)
                except RuntimeError:
                    break
            else:
                reachable_ips.append(join_ip)

        if not reachable_ips:
            raise ClusterConfigurationError(
                'None of the cluster nodes are reachable: {0} (ports: {1})'
                .format(
                    ', '.join(join_ips),
                    ', '.join('{0}'.format(port) for port in REQUIRED_PORTS)
                ))

        self.logger.info('Joining an existing HA cluster with config: {0}'
                         .format(config))
        return self._start_cluster_member(config)

    @admin_only
    def update_config(self, new_config):
        if 'master' in new_config:
            candidate = new_config['master']
            if candidate not in utils.get_online_nodes():
                raise ValueError('{0} is not online!'.format(candidate))
            if candidate == cluster_status_storage.next_master:
                raise ValueError('{0} is already the master'.format(candidate))
            with utils.ConsulLock(database.DB_LOCK_KEY):
                cluster_status_storage.next_master = candidate
                if not utils.wait_for_master(candidate):
                    raise RuntimeError('Switched master but not online: {0}'
                                       .format(candidate))
        return self.cluster_status()

    def list_nodes(self):
        if not node_status.get('initialized'):
            raise RuntimeError('This node is not part of a cluster')

        # for a node to be considered online, it needs to be seen online
        # internally in the cluster, and have a recent enough heartbeat
        online_nodes = utils.get_online_nodes() & utils.get_heartbeat_nodes()

        master = cluster_status_storage.master
        nodes = []

        # nodes that are disconnected while in active minority mode are
        # stored separately
        all_nodes = itertools.chain(
            cluster_status_storage.nodes.items(),
            cluster_status_storage.disconnected_nodes.items()
        )
        for node_name, node_data in all_nodes:
            node_data.update({
                'master': node_name == master,
                'online': node_name in online_nodes
            })
            nodes.append(node_data)
        return ListResponse(items=nodes, metadata={})

    def get_node(self, node_id):
        online_nodes = utils.get_online_nodes()
        node = cluster_status_storage.nodes.get(node_id, {})
        return {
            'name': node_id,
            'online': node_id in online_nodes,
            'master': node_id == cluster_status_storage.master,
            'initialized': node.get('initialized', False),
            'host_ip': node.get('host_ip')
        }

    @admin_only
    def add_node(self, node):
        node_id = node['node_name']
        node_ip = node['host_ip']

        if node_id in cluster_status_storage.nodes or \
                node_id in cluster_status_storage.disconnected_nodes:
            raise ConflictError('Node {0} is already in the cluster'
                                .format(node_id))

        consul_client = consul.get_consul_client()

        _, encryption_key_response = consul_client.kv.get('encryption_key')
        if encryption_key_response:
            encryption_key = encryption_key_response['Value']
        else:
            encryption_key = None

        credentials = {
            'encryption_key': encryption_key
        }
        # create ssl certs for the new node to use
        # we generate the certs and send them back to the client, so that
        # the client can pass them to the new node when joining; in principle
        # the client could generate a CSR and send it to us to sign it, but
        # this would require openssl on the client
        ca_crt, ca_key = ssl.get_ca_cert(consul_client)
        server_cert, server_key = ssl.create_cert(
            ca_crt, ca_key, ip=node_ip,
            common_name='consul_{0}'.format(node_id))

        client_cert, client_key = ssl.create_cert(
            ca_crt, ca_key, ip=node_ip,
            common_name='consul_client_{0}'.format(node_id))

        for filename, key in [(server_cert, 'cert'), (server_key, 'key'),
                              (ca_crt, 'ca'), (client_cert, 'client_cert'),
                              (client_key, 'client_key')]:
            with open(filename) as pem_file:
                credentials[key] = pem_file.read()

        return {
            'name': node_id,
            'host_ip': node_ip,
            'online': False,
            'initialized': False,
            'master': False,
            'credentials': credentials
        }

    @admin_only
    def remove_node(self, node_id):
        # just remove the node registration info from consul. The node will
        # take care of stopping all of its services;
        # make sure to handle the case when the node is not there, to allow
        # removing nodes that only partially joined
        node = cluster_status_storage.nodes.pop(node_id, None)
        cluster_status_storage.db_nodes.pop(node_id, None)
        cluster_status_storage.syncthing_nodes.pop(node_id, None)
        cluster_status_storage.agent_settings.pop(node_id, None)
        disconnected_node = cluster_status_storage.disconnected_nodes.pop(
            node_id, None)
        node = node or disconnected_node
        if not node:
            raise NotFoundError('Node {0} not found in the cluster'
                                .format(node_id))
        node_ip = node.get('host_ip')
        if node_ip:
            minority.unblock_consul({node_ip})
        return node

    def _start_cluster_member(self, config):
        """Run the subprocess that actually configures the cluster member."""
        start_cluster = os.path.join(
            os.path.dirname(sys.executable), 'create_cluster_node')

        if node_status.get('initialized'):
            raise ClusterConfigurationError(
                'Cluster is already initialized on this node!')

        if 'error' in node_status:
            # we're retrying after an error - store the current log position
            # so that we can skip previous logs in future requests
            del node_status['error']
            self.logger.warning('Retrying cluster initialization')
            logs = self._load_logs()
            last_log = logs[-1]
            node_status['last_cursor'] = last_log['cursor']

        with tempfile.NamedTemporaryFile(delete=False) as f:
            json.dump(config, f)

        # instead of running the subprocess directly, we leverage systemd-run
        # for several reasons:
        #   - so that the start_cluster script isn't a child process of
        #     the REST service, which allows it to cleanly restart it
        #   - passing the unit name will make systemd-run disallow multiple
        #     concurrent executions of the script
        #   - stdout is automatically redirected to journald for easy log
        #     querying and error reporting
        systemd.SystemdManaged.run_command(
            CLUSTER_SYSTEMD_UNIT_NAME,
            [start_cluster, '--config', f.name],
            sudo=True)

        return self.cluster_status()

    def _load_logs(self, since=None, **filters):
        if since is not None:
            if len(since) != 1:
                raise ValueError('Malformed cursor "since"')
            after_cursor = since[0]
        else:
            after_cursor = node_status.pop('last_cursor', None)
        service = systemd.SystemdManaged(CLUSTER_SYSTEMD_UNIT_NAME, sudo=True)
        return service.get_journald_logs(after_cursor=after_cursor)

    def _verify_no_data(self):
        """Check that the current manager is empty.

        The manager is considered empty, if there's no blueprints stored
        in the database.
        Note: this method is onyl usable in app context (from the REST API).
        """
        bps = get_storage_manager().list(models.Blueprint, all_tenants=True)
        if bps:
            raise ClusterConfigurationError(
                'Cannot join cluster from a nonempty manager!')
