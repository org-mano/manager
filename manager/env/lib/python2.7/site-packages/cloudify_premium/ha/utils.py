########
# Copyright (c) 2016 GigaSpaces Technologies Ltd. All rights reserved
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#        http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
#    * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#    * See the License for the specific language governing permissions and
#    * limitations under the License.

from __future__ import absolute_import

import os
import grp
import pwd
import json
import stat
import time
import uuid
import yaml
import errno
import jinja2
import socket
import getpass
import hashlib
import logging
import requests
import threading
import subprocess
import pkg_resources
from base64 import standard_b64encode

from consul import NotFound
from collections import MutableMapping
from requests.exceptions import HTTPError
from fasteners.process_lock import InterProcessLock

try:
    import psycopg2
except ImportError:
    psycopg2 = None

STATUS_STORAGE_PATH = '/opt/cloudify/.cluster'
logger = logging.getLogger(__name__)
HEARTBEAT_INTERVAL = 10
RESTSERVICE_CONFIG_PATH = '/opt/manager/cloudify-rest.conf'
CLOUDIFY_USER = 'cfyuser'
RESTSERVICE_OS_USER = 'cfyuser'
CLUSTER_OS_GROUP = 'cluster'


class NodeStatus(MutableMapping):
    """
    Interface for controlling the status file stored on disk.

    Some parts of the cluster status can only be stored on the filesystem,
    because eg. the cluster-wide key-value store hasn't been bootstrapped yet,
    or has encountered an error.
    """
    def __init__(self, filepath=STATUS_STORAGE_PATH):
        self._filepath = filepath
        self._lock = InterProcessLock(
            '/tmp/cloudify-lock-{0}-{1}'
            .format(hashlib.sha1(filepath).hexdigest(), getpass.getuser()))
        # fasteners interprocess locks are actually not threadsafe - we need
        # to also use a threading lock first
        self._internal_lock = threading.RLock()

    def _load(self):
        """
        Return the status loaded from the filesystem storage.
        """
        with self._internal_lock:
            with self._lock:
                try:
                    with open(self._filepath) as f:
                        return json.load(f)
                except IOError as e:
                    if e.errno == errno.ENOENT:
                        return {}
                    raise

    def _store(self, new_data):
        with self._internal_lock:
            with self._lock:
                with open(self._filepath, 'w') as f:
                    json.dump(new_data, f, indent=4, sort_keys=True)

    def __getitem__(self, key):
        return self._load()[key]

    def __setitem__(self, key, value):
        with self._internal_lock:
            data = self._load()
            data[key] = value
            self._store(data)

    def __delitem__(self, key):
        with self._internal_lock:
            data = self._load()
            del data[key]
            self._store(data)

    def __iter__(self):
        return iter(self._load())

    def __len__(self):
        return len(self._load())


class _WithConsulClient(object):
    def __init__(self, consul_client=None, *args, **kwargs):
        super(_WithConsulClient, self).__init__(*args, **kwargs)
        self._consul_client = consul_client

    @property
    def consul_client(self):
        if self._consul_client is None:
            # import here because of circular dependency. This cannot be used
            # at import time anyway - consul needs to actually be configured
            # first.
            from cloudify_premium.ha import consul
            self._consul_client = consul.get_consul_client()
        return self._consul_client


class _ClusterNodes(MutableMapping, _WithConsulClient):
    def __init__(self, consul_client=None, prefix='nodes/'):
        super(_ClusterNodes, self).__init__(consul_client)
        self._prefix = prefix

    def _get_path(self, name):
        return '{0}{1}'.format(self._prefix, name)

    def get_node_with_index(self, name):
        _, response = self.consul_client.kv.get(self._get_path(name))
        if response is None:
            return 0, None
        index = response['ModifyIndex']
        try:
            return index, json.loads(response['Value'])
        except (ValueError, KeyError):
            # treat malformed value as empty
            return index, None

    def __getitem__(self, key):
        _, value = self.get_node_with_index(key)
        if value is None:
            raise KeyError(key)
        return value

    def __setitem__(self, key, new_value):
        index, value = self.get_node_with_index(key)
        if value:
            value.update(new_value)
        else:
            value = new_value
        saved = self.consul_client.kv.put(self._get_path(key),
                                          json.dumps(value), cas=index)
        if not saved:
            raise RuntimeError('Concurrency error: node not saved (index={0})'
                               .format(index))

    def __delitem__(self, key):
        index, value = self.get_node_with_index(key)
        deleted = self.consul_client.kv.delete(self._get_path(key), cas=index)
        if not deleted:
            raise RuntimeError('Concurrency error: not deleted (index={0})'
                               .format(index))

    def _get_keys(self):
        _, response = self.consul_client.kv.get(self._get_path(''),
                                                recurse=True)
        if not response:
            return []
        return [node['Key'].replace(self._prefix, '') for node in response]

    def __iter__(self):
        return iter(self._get_keys())

    def __len__(self):
        return len(self._get_keys())


class _ConsulProperty(object):
    def __init__(self, key, consul_client=None, default=None, json=False):
        self._key = key
        self._consul_client = consul_client
        self._default = default
        self._json = json

    def __get__(self, obj, objtype):
        consul_client = self._consul_client \
            or getattr(obj, 'consul_client', None)
        if consul_client is None:
            raise RuntimeError('No consul client!')

        _, response = consul_client.kv.get(self._key)
        if response is not None:
            val = response['Value']
            if self._json:
                if not val:
                    val = self._default
                else:
                    val = json.loads(val)
        else:
            val = self._default
        return val

    def __set__(self, obj, val):
        consul_client = self._consul_client \
            or getattr(obj, 'consul_client', None)
        if consul_client is None:
            raise RuntimeError('No consul client!')

        if self._json:
            val = json.dumps(val)
        # use index to make sure concurrent writes don't overwrite each other
        _, previous = consul_client.kv.get(self._key)
        if previous is None:
            index = 0
        else:
            index = previous['ModifyIndex']
        saved = consul_client.kv.put(self._key, value=val, cas=index)
        if not saved:
            raise RuntimeError(
                'Concurrency error: {0} not saved (index={1})'
                .format(self._key, index))


class ClusterStatus(_WithConsulClient):
    @property
    def nodes(self):
        return _ClusterNodes(self.consul_client)

    @property
    def db_nodes(self):
        return _ClusterNodes(self.consul_client, prefix='db/nodes/')

    @property
    def agent_settings(self):
        return _ClusterNodes(self.consul_client, prefix='cloudify_agent/')

    @property
    def syncthing_nodes(self):
        return _ClusterNodes(self.consul_client, prefix='syncthing/nodes/')

    # "master" is the current master, "next_master" is used during failover
    # to signal to a node that it should promote itself.
    # Failover workflow:
    #   - any node chooses the new master and sets .next_master to its name
    #   - each node has its next_master watch run and compares the value to
    #     its name - the one that matches starts promoting itself
    #   - after the new master is promoted, it sets the .master to its name
    #   - other nodes fire a watch on .master, and start following it
    # In normal operation, next_master == master
    master = _ConsulProperty('master')
    next_master = _ConsulProperty('next_master')
    db_credentials = _ConsulProperty('db/credentials',
                                     json=True, default={})
    replication_credentials = _ConsulProperty('db/replication_credentials',
                                              json=True, default={})

    @property
    def disconnected_nodes(self):
        return _ClusterNodes(self.consul_client, prefix='disconnected_nodes/')


class ConsulLockTimeoutError(RuntimeError):
    """A timeout occured trying to acquire a distributed lock."""


class ConsulLock(_WithConsulClient):
    """A distributed lock implemented using consul.

    Only one node in the cluster can hold this lock.
    """
    # implementation based on the ideas from:
    # https://www.consul.io/docs/guides/semaphore.html
    # https://www.consul.io/docs/guides/leader-election.html

    # TODO consider adding a blocking= parameter, which is needed to have
    # the same interface as the builtin python threading.Lock
    def __init__(self, name, timeout=5, interval=5, consul_client=None,
                 skip_exit=False):
        super(ConsulLock, self).__init__(consul_client=consul_client)
        self._name = name
        self._timeout = timeout
        self._interval = interval
        self._path = os.path.join('.locks', self._name)
        self.skip_exit = skip_exit
        self._session_name = '{0}_{1}'.format(self._name, uuid.uuid4().hex)
        self._renewing_stopped = threading.Event()

    def acquire(self):
        # we don't pass any checks to the session, so the default serfHealth
        # will be used - the lock will be automatically released when
        # the node holding it dies
        self.session = self.consul_client.session.create(
            name=self._session_name,
            lock_delay=2,
            ttl=10)
        acquired = self._acquire_lock()
        if acquired:
            self._start_renewing()
        else:
            self.consul_client.session.destroy(self.session)
            raise ConsulLockTimeoutError('Timeout: unable to acquire lock {0}'
                                         .format(self._session_name))

    def release(self):
        self._stop_renewing()
        if not self.skip_exit:
            logger.debug('destroy {0}'.format(self._session_name))
            self.consul_client.kv.put(self._path, '', release=self.session)
            self.consul_client.session.destroy(self.session)

    def __enter__(self):
        self.acquire()

    def __exit__(self, exc, value, tb):
        self.release()

    def __repr__(self):
        return '<ConsulLock session={0} host={1}>'.format(
            self._session_name, self.consul_client.http.host)

    def _acquire_lock(self):
        deadline = time.time() + self._timeout
        while time.time() < deadline:
            acquired = self.consul_client.kv.put(
                self._path, node_status['name'], acquire=self.session)
            if acquired:
                logger.debug('Acquired {0!r}'.format(self))
                return True

            ix, response = self.consul_client.kv.get(self._path)
            if response.get('Session'):
                wait_time = '{0}s'.format(self._interval)
                try:
                    self.consul_client.kv.get(self._path, index=ix,
                                              wait=wait_time)
                except requests.exceptions.ReadTimeout:
                    continue
            else:
                time.sleep(self._interval)
        return False

    def _start_renewing(self):
        self._renewing_stopped.clear()

        def _renewer(consul_client, session_id, finished):
            while not finished.is_set():
                try:
                    consul_client.session.renew(session_id)
                except NotFound:
                    return
                except HTTPError:
                    pass
                time.sleep(5)

        self._renewer_thread = threading.Thread(
            name='renewer_{0}'.format(self._session_name),
            target=_renewer,
            args=(self.consul_client, self.session, self._renewing_stopped))
        self._renewer_thread.daemon = True
        self._renewer_thread.start()

    def _stop_renewing(self):
        self._renewing_stopped.set()


class _ClusterOnlineChecker(_WithConsulClient):
    def _get_passing_nodes(self, service):
        """Node names on which the service status is green."""
        _, services = self.consul_client.health.service(service, passing=True)
        return {s['Node']['Node'] for s in services}

    def get_online_nodes(self):
        """Return the names of the nodes that pass all checks."""
        required_services = ['database', 'managerservices']
        checks = [self._get_passing_nodes(service)
                  for service in required_services]

        return set.intersection(*checks)

    def get_heartbeat_nodes(self):
        """Nodes in the cluster that updated their heartbeat counter recently.
        """
        _, heartbeat_response = self.consul_client.kv.get('heartbeat',
                                                          recurse=True)
        if heartbeat_response is None:
            return set()
        recent = set()
        deadline = time.time() - HEARTBEAT_INTERVAL * 3
        for node in heartbeat_response:
            try:
                _, name = node['Key'].split('/')
                last_heartbeat = float(node['Value'])
            except ValueError:
                continue
            if last_heartbeat > deadline:
                recent.add(name)
        return recent

    def is_master_online(self):
        """Is the master node seen as online?

        Check all the relevant services status in consul: currently we need
        to look at the status of the database, and of the manager_services
        service - which is a common name for all the master-only services.
        """
        master = cluster_status.next_master
        return master in self.get_online_nodes()

    def wait_for_master(self, master=None, retries=30, retry_interval=3):
        """Wait until the given node is the master, and passes all checks.

        If the node name isn't provided, just wait until the node currently
        set as master passes all checks.
        """
        for retry in range(retries):
            new_master = master or cluster_status.next_master
            online_nodes = self.get_online_nodes()
            if new_master in online_nodes\
                    and new_master == cluster_status.master:
                return True
            logger.debug('master: {0} not online ({1}/{2})'
                         .format(new_master, retry, retries))
            time.sleep(retry_interval)
        return False

    def get_candidates(self):
        """Alive nodes in the cluster that can be the new master.

        Yields pairs of (node name, xlog position), so that the node with the
        highest xlog position can be chosen as the new master.

        Only check for consul and manager services passing - don't need
        to look up the state of the database, because we're connecting
        to the database anyway for its xlog position, so if it's down,
        we'll be able to skip it then.
        """
        if psycopg2 is None:
            raise RuntimeError('psycopg2 is required to get candidates!')

        alive_services_nodes = self._get_passing_nodes('managerservices')
        reachable_consul_nodes = self._get_passing_nodes('consul')

        candidates = []
        for node_name in (reachable_consul_nodes & alive_services_nodes):
            try:
                candidate = node_name, get_xlog_location(node_name)
            except psycopg2.OperationalError:
                continue
            if candidate[1] == -1:
                continue
            candidates.append(candidate)
        return candidates


def get_xlog_location(node_name):
    """Return the xlog location of the database at host,port.

    This is used when selecting a new master - the node with the highest
    xlog location is selected, which guarantees the leader will be the node
    with the most up-to-date database.
    """
    # TODO fix circular import
    from cloudify_premium.ha.database import database_connection

    try:
        with database_connection(node_name, replication=True) as conn:
            with conn.cursor() as cur:
                cur.execute('IDENTIFY_SYSTEM;')
                results = cur.fetchall()
    except (psycopg2.DatabaseError, KeyError):
        return -1

    pg_lsn = results[0][2]
    timeline, _, location = pg_lsn.partition('/')
    return int(location, 16)


def render_resource(f, resource_name, **context):
    template_source = pkg_resources.resource_string(__name__, resource_name)
    template = jinja2.Template(template_source)
    rendered_data = template.render(**context)
    f.write(rendered_data)


def is_master():
    return cluster_status.master == node_status['name']


def wait_for_port(host, port, retries=20, retry_interval=1):
    """Wait for the port at (host, port) to become open.

    Try retries times to connect, with a retry_interval sleep before each
    try.
    """
    logger.debug('Waiting for {0}:{1}'.format(host, port))
    for retry in range(retries):
        try:
            conn = socket.create_connection((host, port),
                                            timeout=retry_interval)
        except socket.error:
            logger.debug('{0}:{1} not ready yet (retry {2})'
                         .format(host, port, retry))
            time.sleep(retry_interval)
        else:
            conn.close()
            logger.debug('{0}:{1} ready!'.format(host, port))
            return
    raise RuntimeError('{0}:{1} not reachable!'.format(host, port))


def load_restservice_config():
    with open(RESTSERVICE_CONFIG_PATH) as f:
        return yaml.safe_load(f)


def update_restservice_config(new_config):
    config = load_restservice_config()
    config.update(new_config)
    with open(RESTSERVICE_CONFIG_PATH, 'w') as f:
        yaml.safe_dump(config, f)


def cloudify_database_credentials():
    """Load the local database credentials from the REST service config file.

    To access the provider context, we need to query the cloudify database.
    """
    rest_settings = load_restservice_config()
    return {
        'user': rest_settings['postgresql_username'],
        'password': rest_settings['postgresql_password']
    }


def deploy_logrotate(filename, label='cluster'):
    with open(os.path.join('/etc/logrotate.d', label), 'w') as f:
        render_resource(f, 'resources/logrotate.conf', filename=filename)


def generate_encryption_key():
    return standard_b64encode(os.urandom(16))


def create_service_user(user, home, groups=None):
    # note: this is mostly copied from blueprint's utils.py, but with ctx
    # calls removed
    """Creates a user.

    It will not create the home dir for it and assume that it already exists.
    This user will only be created if it didn't already exist.
    """
    if not groups:
        maingroup = user
        groups = []
    else:
        maingroup, groups = groups[0], groups[1:]

    logger.info('Checking whether user {0} exists...'.format(user))
    try:
        pwd.getpwnam(user)
        logger.debug('User {0} already exists...'.format(user))
    except KeyError:
        logger.info('Creating group {group} if it does not exist'
                    .format(group=maingroup))
        # --force in groupadd causes it to return true if the group exists.
        # Other behaviour changes don't affect this basic use of the command.
        subprocess.check_call(['groupadd', '--force', maingroup])

        logger.info('Creating user {0}, home: {1}...'.format(user, home))
        cmd = [
            'useradd',
            '--shell', '/sbin/nologin',
            '--home-dir', home, '--no-create-home',
            '--system',
            '--no-user-group',
            '--gid', maingroup,
            user,
        ]
        if groups:
            cmd.extend(['-G', ','.join(groups)])
        subprocess.check_call(cmd)


def create_cluster_group():
    subprocess.check_call(['groupadd', CLUSTER_OS_GROUP])
    # also add the restservice user to the group - it needs access to cluster's
    # data storage, for api calls
    subprocess.check_call(['usermod', '-G', CLUSTER_OS_GROUP,
                           RESTSERVICE_OS_USER])


def make_file_writable(filename):
    """Make the file writable by the cluster group.
    """
    os.chown(filename, pwd.getpwnam('root').pw_uid,
             grp.getgrnam(CLUSTER_OS_GROUP).gr_gid)
    os.chmod(filename,
             (stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IWGRP |
              stat.S_IROTH))


node_status = NodeStatus()
cluster_status = ClusterStatus()

_online_checker = _ClusterOnlineChecker()
is_master_online = _online_checker.is_master_online
get_candidates = _online_checker.get_candidates
get_online_nodes = _online_checker.get_online_nodes
get_heartbeat_nodes = _online_checker.get_heartbeat_nodes
wait_for_master = _online_checker.wait_for_master
