########
# Copyright (c) 2017 GigaSpaces Technologies Ltd. All rights reserved
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#        http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
#    * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#    * See the License for the specific language governing permissions and
#    * limitations under the License.

"""Implementation of the "active minority" feature.

When the cluster is partitioned, we make sure the minority part of the cluster
is still active (has a master elected and accepts writes). This module has
utility functions that help implement this feature.

Note that the most common usecase is handling one node being down in a cluster
of 2 nodes - in that case, the surviving node is still active.

This defines a "consul watcher" daemon, which keeps polling the local
consul instance and compares how many nodes is it connected to, vs how
many nodes are in the consul cluster. If we're connected to less than the
majority of nodes, we transition to the "active minority" mode. This is
done by dropping the disconnected nodes from the consul cluster.

We also block the disconnected nodes using iptables, so that they don't
reconnect to us automatically. Instead, during the "active minority"
operation, this runs the "consul recovery watcher" daemon, which keeps
trying to connect to the HTTP API of the disconnected nodes, and starts
the rejoin procedure when it succeeds.

During rejoin, we must reconcile the state of the local cluster, with the
state of the remote cluster. Note that if the remote cluster is also a
minority part, it will likely be running the same procedure at the same
time. This takes a lock on BOTH parts of the cluster, so that only one
part actually does it.

During rejoin, we only keep the database from one part of the cluster,
and drop the database from the other. This looks at the last_login_at
column in the users table, and discards the database that was the least
recently used. (in most common use cases, that database was unreachable
by any users, so has had no changes during the split; note however if it
did have some changes during that time, they will be lost)

The rejoining part of the cluster then selectively copies its consul storage
to the remote consul, drops the local consul data directory, and clones
it from the remote part of the cluster. This allows a conflict-free rejoin
of the consul cluster parts.
"""

import time
import socket
import logging

from cloudify_premium.ha import (cluster_status,
                                 consul,
                                 database,
                                 node_status,
                                 sudo,
                                 utils,
                                 watch_handlers)
try:
    from psycopg2 import OperationalError
except ImportError:
    # happens only in tests - if psycopg2 is not available, then we'll get
    # a RuntimeError from database.create_connection anyawy, so
    # OperationalError is not important then
    OperationalError = Exception


# ports used by consul - those aren't configurable, so we can just hardcode
# them here - those are the ports blocked when we need to fence nodes out
CONSUL_RPC_PORT = 8300
CONSUL_SERF_PORT = 8301
CONSUL_HTTP_PORT = 8500

RECOVERY_LOCK_KEY = 'recovery'
RECOVERY_CONFIG = '/tmp/recovery-consul.json'
logger = logging.getLogger(__name__)


def watch_consul(check_interval_seconds, logger):
    """Query consul to see if it's still connected to the majority.

    Periodically check the agent.members endpoint in consul, and if we're
    not connected to the majority of the cluster, start the "active minority"
    mode.
    """
    consul_client = consul.get_consul_client()

    # we're just running as a daemon - this can't be implemented as a consul
    # check, because consul stops running checks if it's not connected to the
    # majority
    while True:
        time.sleep(check_interval_seconds)
        try:
            # use .members() to see how many nodes we're connected to, and
            # check the raft stats to see how many nodes consul tries to
            # connect to. If we're only connected to less than majority of
            # that, start minority mode operation
            members = consul_client.agent.members()
            all_members = _get_members_count(consul_client)
        except Exception:
            # some error in consul - possibly related to restarts
            continue

        connected_addrs = {node['Addr'] for node in members
                           if node['Status'] == 1}

        if len(connected_addrs) * 2 <= all_members:
            logger.warning('Lost majority! We see {0} out of {1} nodes'
                           .format(len(connected_addrs), all_members))
            # TODO consider locking this
            _on_consul_disconnected(consul_client, connected_addrs)


def _get_members_count(consul_client):
    """Return the count of consul nodes in the cluster.

    First, try getting the declared cluster nodes - only if that fails,
    fallback to the raft stats. This needs to handle the following edge cases:
      - node was removed from the cluster, but its consul haven't
        disconnected yet
      - remote consul was disconnected from our, but the raft implementation
        still tries to reconnect to it
    """
    status = utils.ClusterStatus(consul_client)
    try:
        all_members = len(status.nodes)
    except Exception:
        raft_stats = consul_client.agent.self()['Stats']['raft']
        all_members = int(raft_stats['num_peers']) + 1
    return all_members


def _on_consul_disconnected(consul_client, connected_addrs, master_retries=20,
                            master_retry_interval=1):
    _set_consul_subcluster(connected_addrs)
    local_recovery_lock = utils.ConsulLock(RECOVERY_LOCK_KEY,
                                           consul_client=consul_client)
    # retry grabbing the lock - we can timeout if we're also electing
    # the new master at this time
    disconnected_addrs = None
    for retry in range(master_retries):
        try:
            with local_recovery_lock:
                # to allow active operation, we remove the nodes we
                # can't connect to, essentially just creating a smaller
                # cluster (eg. if we had 3 nodes and we disconnected
                # from 2 of them, we're now a cluster of 1 machine)
                disconnected_addrs = _prepare_subcluster(consul_client)
                if disconnected_addrs is None:
                    # someone else rejoined to us while we were waiting on the
                    # locks - it's already connected to our consul, so we drop
                    # our K/V replica and rejoin to the same consul cluster
                    # again
                    local_recovery_lock.skip_exit = True
                    logger.debug('on disconnected: rejoining')
                    _rejoin_consul(consul_client, consul_client)
                if not disconnected_addrs:
                    return
                utils.wait_for_master()

        except Exception:
            logger.exception('Error when preparing subcluster')
            time.sleep(master_retry_interval)
        else:
            break


def _prepare_subcluster(consul_client):
    all_node_names = set(cluster_status.nodes)

    # check if perhaps we're connected again by now - another node might've
    # done the rejoin procedure while we were waiting on the lock
    new_connected_names = {n['Name'] for n in consul_client.agent.members()
                           if n['Status'] == 1}
    if set(all_node_names) == set(new_connected_names):
        logger.warning('rejoined concurrently')
        return None

    disconnected_names = all_node_names - new_connected_names
    disconnected_addrs = {cluster_status.nodes[n]['host_ip']
                          for n in disconnected_names}

    logger.warning('Removing nodes!')
    # the new cluster status - remove all the metadata describing the
    # removed nodes
    block_consul(disconnected_addrs)
    _remove_nodes(disconnected_names)

    return disconnected_addrs


def watch_recovery(logger, poll_delay=3, post_wait=120):
    """Poll the disconnected consul cluster and notify when we reconnect to it.

    Simply wait until any of the addresses stored in the json file is
    reachable, do no verification on the service that runs on that port.
    """
    while True:
        try:
            nodes = dict(cluster_status.disconnected_nodes)
            if not nodes:
                time.sleep(poll_delay)
                continue
        except Exception as e:
            logger.debug('watch_recovery: error getting nodes: {0}'.format(e))
            time.sleep(poll_delay)
            continue

        for node_data in nodes.values():
            addr = node_data['host_ip']
            try:
                conn = socket.create_connection((addr, CONSUL_HTTP_PORT),
                                                timeout=3)
            except socket.error:
                continue
            conn.close()

            consul_client = consul.get_consul_client(host=addr)
            try:
                consul_client.agent.self()
            except Exception:
                continue

            logger.info('Reconnected to {0}'.format(addr))
            # TODO consider locking this
            _recover_cluster(consul_client, addr)
            time.sleep(post_wait)


def _recover_cluster(consul_client, addr, retries=20, retry_interval=1):
    connected_nodes = dict(cluster_status.nodes)
    remote_cluster_status = utils.ClusterStatus(consul_client=consul_client)

    for retry in range(retries):
        try:
            if len(connected_nodes) > len(cluster_status.disconnected_nodes):
                # we're the majority - just make sure the minority knows about
                # all the nodes in our part of the cluster, in case it was
                # disconnected while all of our nodes were added
                logger.debug('_recover_cluster: majority')
                consul.wait_until_consul_ready(client=consul_client)
                with utils.ConsulLock(RECOVERY_LOCK_KEY,
                                      consul_client=consul_client):
                    for local_node, local_node_data in connected_nodes.items():
                        if local_node not in remote_cluster_status.nodes:
                            remote_cluster_status.disconnected_nodes\
                                .setdefault(local_node, local_node_data)
            else:
                # we're the minority - rejoin to the other part of the cluster
                logger.debug('_recover_cluster: minority')
                rejoiner = ClusterRejoiner(addr)
                rejoiner.reconcile_nodes()
        except Exception:
            logger.exception('Error in reconcile_nodes (retry {0}/{1})'
                             .format(retry, retries))
            time.sleep(retry_interval)
        else:
            break


class ClusterRejoiner(object):
    def __init__(self, addr, port=CONSUL_HTTP_PORT):
        self._addr = addr
        self._port = port
        self._local_consul = consul.get_consul_client()
        self._remote_consul = consul.get_consul_client(host=addr, port=port)
        self._remote_cluster_status = utils.ClusterStatus(self._remote_consul)

    def reconcile_nodes(self, rejoin_retries=20, rejoin_retry_interval=1):
        """Reconnect the current sub-cluster to the other part of the cluster.
        """
        self._locks = _ConsulLocks(self._addr, self._local_consul,
                                   self._remote_consul)
        if self._already_done():
            logger.debug('already done - before')
            return
        acquired = self.acquire_locks()
        if not acquired:
            raise RuntimeError('not acquired')
        if self._already_done():
            logger.debug('already done - after')
            return

        # if we actually rejoin to the cluster, the lock of our side will
        # be deleted, so we can skip releasing it; still need to release it
        # if we error out before rejoining
        skip_local_exit = False
        try:
            _add_nodes(cluster_status, self._remote_cluster_status)
            remote_db_lock = utils.ConsulLock(
                database.DB_LOCK_KEY, consul_client=self._remote_consul)
            # lock it - we need to guarantee that while we're joining to
            # the other part of the cluster, no leader election takes place
            with remote_db_lock:
                if self._is_local_db_newer():
                    # if our db is newer, we set ourselves as the master of the
                    # other part, and then rejoin it
                    master = cluster_status.master
                    logger.warning('Local db is newer than remote! {0}'
                                   .format(master))
                    # set both master and next_master - we're already
                    # master, # we don't need to promote on next_master
                    # changed
                    self._remote_cluster_status.next_master = master
                    self._remote_cluster_status.master = master
                    _rejoin_consul(self._local_consul, self._remote_consul)
                    self._wait_for_consul_rejoin()
                    # only release the lock after this returns - when our
                    # health checks in the new cluster converge
                    logger.info('waiting for master')
                    utils.wait_for_master(master)
                    logger.info('master done')
                else:
                    # our db is older - just join to the other part of the
                    # cluster and we'll follow the current master
                    _rejoin_consul(self._local_consul, self._remote_consul)
            logger.info('released')

            skip_local_exit = True
        finally:
            self._locks.release(skip_local_exit)

    def acquire_locks(self, acquire_retries=20, acquire_retry_interval=3):
        for retry in range(acquire_retries):
            try:
                if self._locks.acquire():
                    return True
            except Exception:
                pass  # just retry

            if self._already_done():
                self._locks.release()
                return False

            unblock_consul()
            time.sleep(acquire_retry_interval)

    def _wait_for_consul_rejoin(self, retries=20, retry_interval=1):
        for retry in range(retries):
            if self._already_done():
                logger.debug('Consul rejoined')
                return
            logger.debug('Consul not rejoined yet ({0}/{1})'
                         .format(retry, retries))
            time.sleep(retry_interval)

    def _already_done(self, consul_client=None):
        if consul_client is None:
            consul_client = self._local_consul
        if not any(self._addr == n['Addr']
                   for n in consul_client.agent.members()):
            return False
        try:
            member_ips = {n['host_ip'] for n in cluster_status.nodes.values()}
        except Exception:
            return False
        return self._addr in member_ips

    def _is_local_db_newer(self):
        """The database on which part of the cluster has newer writes in it?

        This just looks at the last activity on the server (decided by the
        newest "last_login_at" in the users table).
        """
        # TODO in the future, we might want to use a smarter algorithm
        local_master = cluster_status.master
        remote_master = self._remote_cluster_status.master

        try:
            local_last_login = _get_last_login(local_master)
        except KeyError:
            logger.debug('No local master last_login_at')
            return False
        remote_last_login = _get_last_login(remote_master,
                                            self._remote_cluster_status)
        logger.debug('local_last_login {0}, remote_last_login {1}'
                     .format(local_last_login, remote_last_login))
        return local_last_login > remote_last_login


def _set_consul_subcluster(connected_addrs):
    """Configure consul so only the passed nodes belong to the cluster.

    This uses the consul recovery mechanism (peers.json) to reconnect to
    the passed nodes (in the trivial case, only the current node is there),
    and fences us out of the disconnected nodes - so they can't rejoin
    automatically and break consul, when they reconnect.
    """
    consul_service = consul.Consul()
    consul_client = consul.get_consul_client()
    consul_service.force_peers(['{0}:{1}'.format(addr, CONSUL_RPC_PORT)
                                for addr in connected_addrs])
    consul_service.update_settings(retry_join=None)
    sudo.run(['restart_consul'])
    # TODO: we're using "100" to mean "retry forever"; we should instead
    # implement a real "retry forever" in wait_until_consul_ready
    consul.wait_until_consul_ready(consul_client, retries=100)


def block_consul(addrs):
    """Disallow the given nodes to connect to consul.

    When the nodes reconnect, they will try to rejoin to consul automatically;
    if there were any writes in the meantime, it would create conflicts and
    put consul in an inconsistent state. We prevent that by fencing out from
    the other nodes, allowing us to merge the data manually.
    """
    # ports that consul connects to automatically
    addrs = set(addrs)
    to_block = addrs - set(node_status.get('fenced', []))
    if addrs:
        sudo.run(['iptables', '--block'] + list(to_block))

    node_status['fenced'] = node_status.get('fenced', []) + list(to_block)


def unblock_consul(node_ips=None):
    """Allow the blocked consul nodes to connect again"""
    fenced = set(node_status.get('fenced', []))
    if not fenced:
        return
    if node_ips is not None:
        node_ips = set(node_ips)
    else:
        node_ips = {node['host_ip'] for node in cluster_status.nodes.values()}
    addrs = node_ips & fenced
    still_blocked = fenced - node_ips
    if addrs:
        sudo.run(['iptables'] + list(addrs))

    node_status['fenced'] = list(still_blocked)


def _remove_nodes(nodes_to_remove):
    """Remove the nodes from the cluster."""
    for node_name in nodes_to_remove:
        # not removing agent settings here - still need to propagate all the
        # node to agents so that they can failover to whichever part of the
        # cluster is reachable for them
        node_details = cluster_status.nodes.pop(node_name)
        del cluster_status.syncthing_nodes[node_name]
        del cluster_status.db_nodes[node_name]

        cluster_status.disconnected_nodes[node_name] = node_details


def _add_nodes(local_cluster_status, remote_cluster_status):
    """Merge local nodes into the remote consul kv.

    This will add the metadata about the nodes in our cluster, to the remote
    cluster's storage. This also means that nodes that were joined during
    the partition, will be joined successfully.
    """
    # note: this does no updates, only inserts the nodes that are missing
    for storage_source in ['nodes', 'db_nodes', 'syncthing_nodes']:
        local_nodes = getattr(local_cluster_status, storage_source)
        remote_nodes = getattr(remote_cluster_status, storage_source)
        for node_name, data in local_nodes.items():
            remote_nodes.setdefault(node_name, data)

    for node_name in local_cluster_status.nodes:
        remote_cluster_status.disconnected_nodes.pop(node_name, None)


class _ConsulLocks(object):
    """Acquire the 'recovery' lock on both cluster partitions."""
    # this is required so that both parts of the cluster don't try to start
    # the rejoin procedure; this only happens if both parts are the minority,
    # so in the 1+1 case, not in a 2+1 cluster

    def __init__(self, wait_addr, local_client, remote_client):
        self._wait_addr = wait_addr
        self.local_client = local_client
        self.remote_client = remote_client

        self._locks = None
        self._acquired = []
        self._local_lock = utils.ConsulLock(RECOVERY_LOCK_KEY)
        self._remote_lock = utils.ConsulLock(RECOVERY_LOCK_KEY,
                                             consul_client=self.remote_client)

    def acquire(self):
        logger.debug('getting consul locks')
        if self._locks is None:
            self._locks = self._order_locks()
        for lock in self._locks:
            if lock in self._acquired:
                continue
            acquired = self._acquire_one_lock(lock)
            if acquired:
                self._acquired.append(lock)
            else:
                return False
        return True

    def release(self, skip_local_exit=False):
        self._local_lock.skip_exit = skip_local_exit
        for lock in reversed(self._acquired):
            self._release_one_lock(lock)
            self._acquired.remove(lock)

    def _acquire_one_lock(self, lock, retries=20, retry_interval=1):
        """Acquire the lock, retrying if necessary."""
        for retry in range(retries):
            try:
                consul.wait_until_consul_ready(lock.consul_client)
            except Exception:
                continue  # just retry
            else:
                break
        try:
            lock.acquire()
            return True
        except utils.ConsulLockTimeoutError:
            return False

    def _release_one_lock(self, lock, retries=20, retry_interval=1):
        for retry in range(retries):
            try:
                consul.wait_until_consul_ready(lock.consul_client)
                lock.release()
            except Exception:
                continue  # just retry
            else:
                return
        else:
            raise RuntimeError('Wasnt able to release {0}'.format(lock))

    def _order_locks(self):
        # to avoid deadlocks, both parts need to attempt to acquire the locks
        # in the same order (the actual order isn't important, just that it's
        # the same in both parts). For that, order the locks according to the
        # name of the master node of that cluster part
        local_master = utils.ClusterStatus(self.local_client).master
        remote_master = utils.ClusterStatus(self.remote_client).master

        logger.info('local_master {0} remote_master {1}'
                    .format(local_master, remote_master))
        if local_master > remote_master:
            first_lock, second_lock = self._local_lock, self._remote_lock
        else:
            first_lock, second_lock = self._remote_lock, self._local_lock
        return first_lock, second_lock


def _get_last_login(node, cluster_status=cluster_status, retries=5,
                    retry_interval=1):
    """Get the most recent last user login date from the db on host."""
    for retry in range(retries):
        try:
            with database.database_connection(
                    node, cluster_status=cluster_status,
                    **cluster_status.db_credentials) as conn:
                with conn.cursor() as cur:
                    cur.execute('SELECT get_last_login()')
                    return cur.fetchone()[0]
        except OperationalError:
            logger.info('retrying _get_last_login for node {0} ({1}/{2})'
                        .format(node, retry, retries))
            time.sleep(retry_interval)
    raise


def _rejoin_consul(local_consul_client, remote_consul_client):
    """Stop the local consul, and join the majority's consul cluster.

    When entering the "active minority" mode, we created a single-node
    cluster for our consul; now we need to reconnect back to the majority:
    revert the setting changes, drop the fencing.
    """
    remote_members = remote_consul_client.agent.members()

    consul_service = consul.Consul()
    sudo.run(['minority_rejoin'])

    consul_service.update_settings(retry_join=[
        '{0}:{1}'.format(n['Addr'], n['Port'])
        for n in remote_members
    ])

    unblock_consul({n['Addr'] for n in remote_members})

    for handler_cls in watch_handlers.WATCH_HANDLERS:
        handler = handler_cls()
        handler.stored_value = None
        handler.stored_index = None
    sudo.run(['minority_rejoin', '--start'])

    # TODO: we're using "100" to mean "retry forever"; we should instead
    # implement a real "retry forever" in wait_until_consul_ready
    consul.wait_until_consul_ready(local_consul_client, retries=100)
