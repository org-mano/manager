########
# Copyright (c) 2016 GigaSpaces Technologies Ltd. All rights reserved
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#        http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
#    * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#    * See the License for the specific language governing permissions and
#    * limitations under the License.

"""Handlers run in response to various events in the cluster.

This module defines a "handler runner", which is a daemon that accesses
consul's HTTP API to watch for changes in the K/V and the service storage,
and runs the appropriate handler when a value changes.
We prefer to use a separate daemon instead of consul's built-in watch
handlers for the following reasons:
  - better consistency: this daemon will run the handlers once every N
    seconds, while consul sometimes (especially during rejoins) takes
    a longer (unspecified) amount of time
  - parallelism - we're able to run each watch in a separate thread, so
    multiple handlers can run at the same time (note: they're mostly I/O
    bound; please note python's threading model if you implement a CPU-bound
    handler)
  - reliability - consul can skip running a handler in edge cases during
    rejoins - if before a rejoin the value was the same as after rejoining?
    (needs research to confirm the exact behaviour; by "rejoin" we mean
    what happens in the minority module's `rejoin_consul` procedure)

The daemon uses WatchCommand subclasses that are defined in the WATCH_HANDLERS
list, and runs a thread for each - it keeps polling consul (using the
long polling mode) for the values, and when it receives a value that is
different than the previous (stored in node_status), it runs the registered
handlers.

Handlers are registered as setuptools entry points, using the name from
the appropriate WatchCommand subclass, .cluster_next_master_changed -
users can define their own handlers in their python packages, using the
following entry point names:

  - cluster_nodes_changed - runs when the list of all nodes in the cluster
    changes (eg. when a new node joined the cluster)
  - cluster_next_master_changed - when a new master has been elected, but
    has not finished promoting yet; only the new master node should react
    to this event, and do the work necessary to promote itself to the master
  - cluster_master_changed - when a new master has been elected and it has
    successfully promoted itself to the master; all replica nodes should
    listen to this event, and follow the new master
  - cluster_db_status_changed - status of the database service on one of
    the nodes has changed; the handler function will be called with only
    the passing (ie. alive) nodes
  - cluster_consul_changed - same as the database service, but for the
    consul service
  - cluster_manager_services_changed - same as the database service, but
    for other manager services (rabbitmq, mgmtworker, restservice, etc.)
  - cluster_agent_settings_changed - the agent settings have changed, so
    all connected agents should be updated with the new settings; usually
    only happens when a new node joins the cluster
  - cluster_syncthing_changed - syncthing settings have changed, so all nodes
    should make sure they replicate the given directories; this is usually
    only called when a new node joins the cluster

All handlers should be comptabile with the following signature:
    f(new_value, old_value, **, logger)

"""

import os
import abc
import json
import time
import logging
import tempfile
import threading
import pkg_resources
import logging.config

from contextlib import contextmanager
from fasteners.process_lock import InterProcessLock

from cloudify_premium.ha import (consul,
                                 logger_config,
                                 node_status,
                                 systemd,
                                 utils)


logger = logging.getLogger(__name__)


class WatchCommand(utils._WithConsulClient):
    """Base class for consul watch handlers.

    Commands deriving from this class will be used as handler commands with
    consul to react to changes in KV/services status.
    """
    __metaclass__ = abc.ABCMeta

    def __init__(self, locked=True, *args, **kwargs):
        super(WatchCommand, self).__init__(*args, **kwargs)
        self._logger = None
        self._locked = locked

    @abc.abstractproperty
    def name(self):
        """Name of the change handler group that will be invoked.

        When the value is changed, functions registered in the setuptools
        entry point group of this name will be run.
        """

    @abc.abstractmethod
    def fetch(self):
        """Get data for this watch from consul and parse it"""

    @property
    def logger(self):
        if self._logger is None:
            self._logger = logging.getLogger('commands.{0}'.format(self.name))
        return self._logger

    @property
    def lock(self):
        if self._locked:
            lock_path = os.path.join(
                tempfile.gettempdir(), '{0}.lock'.format(self.name))
            self._lock = InterProcessLock(lock_path)
        else:
            # we're not using a lock, but still keep the same interface:
            # return a no-op contextmanager that can be used the same way
            # as a lock
            @contextmanager
            def _noop():
                yield
            self._lock = _noop()
        return self._lock

    @property
    def stored_value(self):
        """Return the previous value of this handler.

        The previous value will be compared against the current: change
        handlers will only be run when they differ.
        """
        # we store the previous values in node_status
        return node_status.get(self.name)

    @stored_value.setter
    def stored_value(self, new_value):
        node_status[self.name] = new_value

    @property
    def stored_index(self):
        return node_status.get(self.name + '_index')

    @stored_index.setter
    def stored_index(self, new_value):
        node_status[self.name + '_index'] = new_value

    def __call__(self, new_value, old_value):
        with self.lock:
            self._run_handlers(new_value, old_value)

    def _run_handlers(self, *args, **kwargs):
        """The value has changed - run the change handlers.

        Change handler functions are registered using setuptools entry
        points. They can optionally define .retries and .priority attributes.
        """
        kwargs.setdefault('logger', self.logger)
        handlers = {}
        for ep in pkg_resources.iter_entry_points(self.name):
            name, handler = ep.name, ep.load()
            handlers[name] = handler

        if not handlers:
            return

        retries = max(getattr(handler, 'retries', 10)
                      for handler in handlers.values()) or 1
        retry_interval = 1

        # sort the handlers according to priority - unspecified means 0
        handlers_to_run = sorted(handlers, reverse=True,
                                 key=lambda k: getattr(handlers[k],
                                                       'priority', 0))

        # run all handlers; gather those that failed and require a retry into
        # a list, and in the next iteration, run just the failed ones; repeat
        # for retries
        for retry_num in xrange(retries):
            to_retry = []
            for name in handlers_to_run:
                self.logger.debug('Running {0}'.format(name))
                handler = handlers[name]
                try:
                    handler(*args, **kwargs)
                except Exception as e:
                    self.logger.debug('{0} error'.format(name))
                    self.logger.exception(e)
                    # retry for the declared amount of times, or 10 by default
                    if retry_num < getattr(handler, 'retries', 10):
                        to_retry.append(name)
                else:
                    self.logger.debug('{0} done'.format(name))
            if not to_retry:
                break
            # in the next iteration, run just the failed ones
            handlers_to_run = to_retry
            time.sleep(retry_interval)

        return handlers_to_run


# Watch handlers

class NodesWatchCommand(WatchCommand):
    """Command run from a watch on the nodes/ directory in KV."""
    name = 'cluster_nodes_changed'

    def fetch(self):
        ix, data = self.consul_client.kv.get('nodes/', recurse=True,
                                             index=self.stored_index)
        if data is None:
            return ix, None
        return ix, sorted([json.loads(n['Value']) for n in data])


class MasterWatchCommand(WatchCommand):
    """Command run from a watch on the master key in KV."""
    name = 'cluster_master_changed'

    def fetch(self):
        ix, data = self.consul_client.kv.get('master', index=self.stored_index)
        if not data:
            return ix, data
        return ix, data['Value']


class ServiceWatchCommand(WatchCommand):
    """Base class for service watchers, only returns the passing services."""

    def fetch(self):
        ix, data = self.consul_client.health.service(
            self.service_name, index=self.stored_index, passing=True)
        if data is None:
            return ix, data
        return ix, sorted([s['Node']['Node'] for s in data])


class DBServiceWatchCommand(ServiceWatchCommand):
    """Command run from a watch on the database service status."""
    name = 'cluster_db_status_changed'
    service_name = 'database'


class ConsulServiceWatchCommand(ServiceWatchCommand):
    """Command run from a watch on the consul service status."""
    name = 'cluster_consul_changed'
    service_name = 'consul'


class SyncthingWatchCommand(WatchCommand):
    """Command run from a watch on the nodes/syncthing/ directory in KV."""
    name = 'cluster_syncthing_changed'

    def fetch(self):
        ix, data = self.consul_client.kv.get('syncthing/nodes/', recurse=True,
                                             index=self.stored_index)
        if data is None:
            return ix, None
        return ix, sorted([n['Value'] for n in data])


class ManagerServicesWatchCommand(ServiceWatchCommand):
    url = '/health/service/managerservices'
    name = 'cluster_manager_services_changed'
    service_name = 'managerservices'


class AgentSettingsWatchCommand(NodesWatchCommand):
    name = 'cluster_agent_settings_changed'

    def fetch(self):
        ix, data = self.consul_client.kv.get('cloudify_agent/', recurse=True,
                                             index=self.stored_index)
        if data is None:
            return ix, None
        return ix, sorted([n['Value'] for n in data])


class NextMasterWatch(MasterWatchCommand):
    name = 'cluster_next_master_changed'

    def fetch(self):
        ix, data = self.consul_client.kv.get('next_master',
                                             index=self.stored_index)
        if not data:
            return ix, None
        return ix, data['Value']


WATCH_HANDLERS = [
    NodesWatchCommand,
    MasterWatchCommand,
    DBServiceWatchCommand,
    ConsulServiceWatchCommand,
    SyncthingWatchCommand,
    ManagerServicesWatchCommand,
    AgentSettingsWatchCommand,
    NextMasterWatch
]


class Handlers(object):
    def __init__(self):
        self._threads = None
        self._handlers = WATCH_HANDLERS

    def run(self):
        if self._threads is not None:
            raise RuntimeError('Already started!')
        self._threads = [threading.Thread(target=self._run_handler,
                                          args=(handler,))
                         for handler in self._handlers]
        for thread in self._threads:
            thread.setDaemon(True)
            thread.start()
        while True:
            for handler, thread in zip(self._handlers, self._threads):
                if not thread.is_alive():
                    raise RuntimeError('Thread {0} dead'.format(handler))
            time.sleep(1)

    def _run_handler(self, handler_cls):
        client = consul.get_consul_client(timeout=None)
        handler = handler_cls(consul_client=client)
        while True:
            try:
                index, data = handler.fetch()
                handler.stored_index = index
            except Exception:
                logger.debug('err when fetching {0}'.format(handler))
                time.sleep(3)
                handler.stored_index = None
                continue
            if data is not None and data != handler.stored_value:
                logger.info('{0} {1}'.format(handler.name, data))
                handler(data, handler.stored_value)
                handler.stored_value = data


def handler_runner():
    logging.config.dictConfig(
        logger_config.make_logger_config('handler_runner'))
    handlers = Handlers()
    handlers.run()


class HandlerRunner(systemd.SystemdManaged):
    unit_source = 'resources/cloudify-handler-runner.service'
    service_name = 'handler-runner'
    user = utils.CLOUDIFY_USER
