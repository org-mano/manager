########
# Copyright (c) 2016 GigaSpaces Technologies Ltd. All rights reserved
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#        http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
#    * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#    * See the License for the specific language governing permissions and
#    * limitations under the License.

"""Handlers run in response to various events in the cluster.

Those functions are called by consul as watch handlers, using click commands.
"""

import time
import subprocess

from cloudify_premium.ha import (agents,
                                 checks,
                                 cluster_status,
                                 consul,
                                 minority,
                                 node_status,
                                 database,
                                 sudo,
                                 syncthing,
                                 utils)


def handler(retries=10, priority=0):
    """Mark a handler with this decorator to alter the default retrying policy.

    You can also force a handler to run before or after others by changing
    the priority value (lower runs first; all default handlers have a priority
    value of 0).
    """
    def _deco(f):
        f.retries = retries
        f.priority = priority
        return f
    return _deco


def follow_master_db(new_master, old_master, logger):
    retries = 20
    retry_interval = 1

    db = database.Database()
    if node_status['name'] != new_master:
        for retry in range(retries):
            try:
                logger.info('switch_master_db: following')
                db.replication.standby_follow(cluster_status.master)
            except Exception as e:
                logger.warning('Error when switching db: {0} (retry {1}/{2})'
                               .format(e, retry, retries))
                time.sleep(retry_interval)
                continue
            else:
                # we successfully followed the new master - let's mark the db
                # checks passing right here, to avoid waiting for the next
                # checks iteration
                checks.check_local_db.mark_passing()
                checks.check_db_following.mark_passing()
                break


def promote_master_db(new_master, old_master, logger):
    db = database.Database()
    if node_status['name'] == new_master:
        if node_status.get('db') != 'master':
            logger.info('promote_master_db: promoting')
            db.replication.standby_promote()

        cluster_status.master = node_status['name']
        # we just finished promoting - this means our database is of
        # course in a good state - let's mark the checks passing right
        # here to avoid waiting for the next checks iteration
        checks.check_local_db.mark_passing()
        checks.check_db_following.mark_passing()


def start_master_services(new_master, old_master, logger, **kwargs):
    """If this node is the new master, but was a replica before, start the
    services.
    """
    if new_master != node_status['name']:
        return
    logger.info('toggle_master_services: promoting')
    try:
        sudo.run(['promote'])
    except subprocess.CalledProcessError as e:
        logger.warning(e.output)
        services_started = True
    else:
        logger.info('toggle_master_services: promoted')
        services_started = False
    try:
        sudo.run(['nginx', '--allow'])
    except subprocess.CalledProcessError:
        nginx_updated = False
    else:
        nginx_updated = True
    if services_started and nginx_updated:
        checks.check_manager_services.mark_passing()


def stop_master_services(new_master, old_master, logger, **kwargs):
    """If this node is not the new master, stop the services.
    """
    if new_master == node_status['name']:
        return
    logger.info('toggle_master_services: stopping')
    master_ip = cluster_status.nodes[new_master]['host_ip']
    try:
        sudo.run(['follow', '--master', master_ip])
    except subprocess.CalledProcessError as e:
        logger.warning(e.output)
        services_stopped = True
    else:
        logger.info('toggle_master_services: stoped')
        services_stopped = False
    try:
        sudo.run(['nginx', '--location', 'rest-location-locked.cloudify'])
    except subprocess.CalledProcessError:
        nginx_updated = False
    else:
        nginx_updated = True
    if services_stopped and nginx_updated:
        checks.check_manager_services.mark_passing()


def disable_consul_bootstrap(new_cluster, old_cluster, **kwargs):
    if not old_cluster:
        return
    # consul bootstrap is on when first starting the cluster, only on the
    # master; disable it when the cluster grows from 1 to 2 nodes
    # (old=1, new=2).
    # (this will also run when the cluster is reduced back to 1 node and
    # then grows again, but consul.disable_bootstrap() is idempotent)
    initialized = sum(1 for n in new_cluster if n['initialized'])
    old_initialized = sum(1 for n in old_cluster if n['initialized'])
    if old_initialized == 1 and initialized == 2 and utils.is_master():
        consul_service = consul.Consul()
        consul_service.disable_bootstrap()


def update_db_settings(new_cluster, old_cluster, logger, **kwargs):
    data_dir = node_status['postgresql_data_dir']

    logger.info('Cluster changed: update database settings')
    db_config = node_status['db_config']
    command = [
        'postgresql',
        '--data-dir', data_dir,
        '--cloudify-db', db_config['cloudify_db'],
        '--cloudify-user', db_config['cloudify_user'],
        '--replication-user', db_config['replication_user']
    ]
    command += [node['host_ip'] for node in cluster_status.nodes.values()]
    sudo.run(command)
    logger.info('Cluster changed: update complete')


@handler(retries=30)
def choose_new_master(new_nodes, old_nodes, logger, **kwargs):
    if not new_nodes:  # nothing to failover to - skip
        return
    # if the master already is online, do nothing; also check again after
    # each time-consuming operation (grabbing the lock, finding candidates)
    if utils.is_master_online() and cluster_status.master in new_nodes:
        return
    logger.debug('Master seems down - starting failover')
    with utils.ConsulLock(database.DB_LOCK_KEY):
        logger.debug('Acquired lock')
        if utils.is_master_online():
            return

        candidates = utils.get_candidates()
        logger.debug('Candidates: {0}'.format(candidates))
        if not candidates:
            raise RuntimeError('No candidates')

        best_candidate = max(candidates, key=lambda x: x[1])[0]
        logger.info('Best master candidate: {0}'.format(best_candidate))

        if utils.is_master_online():
            logger.info('Master is online - nothing to do')
            return

        logger.info('Setting new master: {0}'.format(best_candidate))
        cluster_status.next_master = best_candidate
        database.wait_for_postgres(best_candidate)

        if not utils.wait_for_master(best_candidate):
            raise RuntimeError('Switched master but not online: {0}'
                               .format(best_candidate))
    logger.debug('db_online_handler done')


def update_syncthing_config(new_nodes, old_nodes, logger, **kwargs):
    logger.debug('updating syncthing config')
    syncthing.update_devices()
    logger.debug('syncthing config updated')


def update_agents(_new, _old, logger, **kwargs):
    """The cluster has changed: make sure all agents have the current list.

    Run this even on the replicas, even though they shouldn't have rabbitmq
    running - if they do, we better make sure that the agents connected
    to that rabbitmq have the current cluster nodes list, so that they can
    switch to the master.

    Note that this runs both on agent settings changed, and on master changed.
    """
    if utils.is_master():
        agents.update_agent_context()

    agents.send_update()


def remove_consul_block(new_cluster, old_cluster, logger, **kwargs):
    """If we're blocking a node that was added to the cluster, unblock it.

    During a rejoing from a 1-1 split (neither part has majority), the remote
    partition will add their node data to our consul, when it's ready to
    rejoin the consul cluster. We need to stop blocking it so that it may
    rejoin.
    """
    minority.unblock_consul()


def disconnect_from_cluster(new_cluster, old_cluster, logger, **kwargs):
    if old_cluster is None:
        return
    removed_nodes = {node['name'] for node in old_cluster} - {
        node['name'] for node in new_cluster}

    if not removed_nodes:
        return

    if node_status['name'] in removed_nodes:
        # this runs on the machine that was removed from the cluster, if it
        # is still online - disable everything, this node won't be able
        # to return to the cluster
        logger.critical('We were forcibly removed from the cluster!')

        sudo.run(['nginx', '--location', 'rest-location-left.cloudify'])
        sudo.run(['leave'])
    else:
        # this runs on the machines that are left in the cluster

        # in case the remote node was not online, and wasn't able to remove
        # itself from the cluster - simply force_leave it to forget about
        # that node
        consul_client = consul.get_consul_client()
        for node in removed_nodes:
            consul_client.agent.force_leave(node)
